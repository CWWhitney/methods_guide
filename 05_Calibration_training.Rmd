## Calibration training

The next step of the modeling procedure follows procedures, outlined by @Hubbard_2014, known as ‘calibration training’. Experts are trained, using well-established ‘calibration’ procedures to estimate their state of uncertainty and thereby increase their capacity to provide accurate estimates by reducing errors of judgment. Through the process, experts learn to improve their ability to estimate their own state of uncertainty and thereby reduce errors of judgment, e.g. under-confidence or overconfidence [i.e. give correct estimates 90% of the time when they have 90% confidence, @Hubbard_2014]. As both @Hubbard_2014 and @Ohagan_2006 show, through this training the experts learn to minimize potential biases in probability estimation. 

Calibration training consists of several exercises aiming to reveal to the participants their personal biases (overconfidence or underconfidence) by assessing their performance on a set of trivia questions. Through these exercises experts are trained to assess their subjective uncertainty and express it as a Confidence Interval (CI). This interval has a predefined chance (e.g. 90%) of containing the right value.  Perfectly calibrated people should get around 90% of answers correct, and any deviation (outside a narrow band of stochastic variation) from this optimal figure indicates estimation bias. Experts are calibrated through repetition and feedback on the exercises. The training is divided into several stages:

\begin{enumerate}
  \item The concepts of calibration are presented to the participants, along with the empirical evidence that assessing uncertainty is not a skill that arises automatically from experience but a general skill that can be gained through training in quantification techniques.
\item Participants benchmark their “natural” skills in quantifying their own uncertainty. To this end, they take a short quiz aimed to help them determine their initial level of overconfidence.
\item Methods of self-calibration proposed by Hubbard (2014), are provided such as: 
\begin{itemize}
  \item The “equivalent bet test”: Participants are invited to imagine a spinning dial on a circle that is 10% red and 90% green. They are asked to imagine that if the real answer falls within the range they selected, they win 100 dollars, or they can choose to spin the dial and if the arrow lands on green are they win 100 dollars. If they prefer the dial over the wheel they do not have 90% confidence in their answer (overconfident) and if they choose their answer instead of the wheel, their confidence level is higher than 90% (underconfident). 
  \item The test of “considering two pros and two cons”: Participants are invited to imagine that the limits that they provided for their estimate range are wrong. They then consider different possible explanations for thy these are wrong and, if any insights arise, they adjust accordingly.  
  \item The “absurdity test”: Participants are invited to create distributions that they are sure are wildly broader than the actual 90% confidence range. They then slowly reduce their ranges down to a more reasonable distribution while considering the logic behind the reduction.
  \end{itemize}
\item Participants work on two types of exercises: (1) 90% CI questions and (2) binary questions. In the 90% CI questions, they are asked to provide a range (a lower bound and an upper bound) for which there is a 90% chance that it includes the right answer. In the binary questions, they answer whether each of a series of statements is true or false and then give the probability they think their answer is the right one. This is an iterative process: they give their answers, compare with the true values and test again.
\end{enumerate}

The examples given by @Hubbard_2014 and @Luedeling_Wajir_2015 show that through the use of these procedures, experts learn to give estimates that are neither too vague (under-confidence) or too specific (overconfidence). The procedures seek to help participants become accurate estimators. Accuracy, in this context, does not mean precision but rather refers to the ability to produce an accurate range of probable estimates of different possible states of the variables of interest. Therefore, the procedures outlined in this appraoch seek to provide experts with the skills necessary to represent uncertainty explicitly as probabilities of different possible states of the world. 

Estimations of the nodes states within the BN can begin once the experts have been through the training and the analysts are satisfied that the experts are adequately calibrated. More examples of these calibration procedures and instructions on their application is provided in detail by @Hubbard_2014.

